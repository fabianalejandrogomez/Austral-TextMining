{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EjemploClasificacionFakeNews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMvDWHo44vjP"
      },
      "source": [
        "# Ejemplo de clasificacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAuvaHjZUNNe"
      },
      "source": [
        "## Importar librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqnR8sM_UMfA"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI3hzf8c4_sN"
      },
      "source": [
        "## Exploracion Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciRdTY8QtFXM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_colwidth',300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6eAo9fM5IWF"
      },
      "source": [
        "# Importo el dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Austral/TextMining/Clases/Clase3/Practica/datos/archive.zip', encoding='utf-8') #Encoding\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S08VG-UHOSQ1"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHRm9S_itmp5"
      },
      "source": [
        "print(\"Cantidad de targets: \")\n",
        "df.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10-ygXeHCWw4"
      },
      "source": [
        "# Creo una caracteristica con el tamaño del titulo\n",
        "df['title_text_size'] = df['title'].str.len()\n",
        "df['title_text_words_count'] = df['title'].str.split().apply(len)\n",
        "df['text_text_size'] = df['text'].str.len()\n",
        "df['text_text_words_count'] = df['text'].str.split().apply(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXhG_xYsuO2G"
      },
      "source": [
        "import plotly.express as px\n",
        "fig = px.histogram(df, x=\"title_text_words_count\", color='label',histnorm='probability')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXTg_VJO73nq"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def get_top_n_words(corpus, n=10):\n",
        "    cv = CountVectorizer() # Agregar StopWords\n",
        "    corpus_matrix = cv.fit_transform(corpus)\n",
        "    corpus_matrix = pd.DataFrame.sparse.from_spmatrix(corpus_matrix, columns=cv.get_feature_names())\n",
        "    aux = corpus_matrix.sum()\n",
        "    aux=aux.to_frame('count')\n",
        "    aux.sort_values(by='count',ascending=False,inplace=True)\n",
        "    aux=aux.head(n)\n",
        "    return aux"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GMZfcN7CjRC"
      },
      "source": [
        "# Cantidad de tokens en todo el corpus\n",
        "most_importants = get_top_n_words(df.text, n=500)\n",
        "print(most_importants.head(10))\n",
        "px.bar(most_importants, x=most_importants.index, y='count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61uLLn7O1nb7"
      },
      "source": [
        "# Cantidad de tokens en todo el corpus\n",
        "most_importants = get_top_n_words(df[df.label=='FAKE'].text, n=500)\n",
        "print(most_importants.head(10))\n",
        "px.bar(most_importants, x=most_importants.index, y='count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7bBSDeD19h0"
      },
      "source": [
        "# Cantidad de tokens en todo el corpus\n",
        "most_importants = get_top_n_words(df[df.label=='REAL'].text, n=500)\n",
        "print(most_importants.head(10))\n",
        "px.bar(most_importants, x=most_importants.index, y='count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibms-RhsEbjb"
      },
      "source": [
        "## Proceso NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO2AO7wRDVWl"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "stop_words = set( stopwords.words('english'))\n",
        "stop_words.update(string.punctuation)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Pre procesamiento basico, agregar las etapas que considere necesario\n",
        "def pre_procesamiento_texto(text):\n",
        "  # Armo los tokens para procesar los datos\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Elimino las stopwords\n",
        "  tokens = [t.lower() for t in tokens if t.lower() not in stop_words]\n",
        "\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCYat43tD1Ar"
      },
      "source": [
        "text='Daniel Greenfield, a Shillman Journalism Fellow at the Freedom Center, is a New York writer focusing on radical Islam'\n",
        "print(text)\n",
        "pre_procesamiento_texto(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7ZZiC74x4DS"
      },
      "source": [
        "## Vectorizacion resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE1Cs5_gD6Ph"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GtbLgMTJMpU"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.text, df.label,test_size=.30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WkJmfOqIorM"
      },
      "source": [
        "# Vectorizacion de resultados BOW, tambien se puede probar TF-IDF\n",
        "cv = CountVectorizer(tokenizer=pre_procesamiento_texto)\n",
        "X_train_transform = cv.fit_transform(X_train) # Armo matriz para entrenar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AowB_A800Jkm"
      },
      "source": [
        "X_train_transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwUlVz29S-Dg"
      },
      "source": [
        "df_tranform = pd.DataFrame.sparse.from_spmatrix(X_train_transform, columns=cv.get_feature_names())\n",
        "df_tranform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmuFUd2BlGQH"
      },
      "source": [
        "aux = df_tranform.sum().to_frame('count')\n",
        "aux.sort_values(by='count',ascending=False,inplace=True)\n",
        "aux=aux.head(1000)\n",
        "px.bar(aux, x=aux.index, y='count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9IyaU-Y88Ew"
      },
      "source": [
        "print(\"Vocabulario de \", len(cv.get_feature_names()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBv7KTdtLSGd"
      },
      "source": [
        "X_train_transform.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLkKF7w5Rqa3"
      },
      "source": [
        "Se puede ver que tenemos mas features que observaciones. Si se utiliza algun metodo de algebra lineal se necesitan tener al menos la misma cantidad de observaciones que de incognitas, sino existiran infinitas soluciones haciendo que el modelo no generalize correctamente. \n",
        "\n",
        "Es recomendable utilizar tecnicas de Cross \n",
        "Validation,  reduccion de dimencionalidad y seleccion de features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEi0st59Efp9"
      },
      "source": [
        "## Modelo Regresion Logistica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeeQ3F8iJ8gA"
      },
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(X_train_transform, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFH8353BEkm-"
      },
      "source": [
        "### Resultados obtenidos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwcvdgn1KDFT"
      },
      "source": [
        "X_test_tranform = cv.transform(X_test) \n",
        "y_pred = clf.predict(X_test_tranform.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZDEu3XsKQNB"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Youyd5YcKnJR"
      },
      "source": [
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdpspo0OkNoT"
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve,plot_confusion_matrix\n",
        "plot_roc_curve(clf,X_test_tranform,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_w415wJ24Nz"
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve,plot_confusion_matrix\n",
        "plot_confusion_matrix(clf,X_test_tranform,y_test,normalize='true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMVAGT-D5Pi5"
      },
      "source": [
        "features = pd.DataFrame()\n",
        "features['feature'] = cv.get_feature_names()\n",
        "features['importance'] = clf.coef_.flatten()\n",
        "\n",
        "features = features.sort_values(by='importance')\n",
        "\n",
        "features_top_positivas = features[:200]\n",
        "features_top_negativas = features[-200:]\n",
        "\n",
        "px.bar(pd.concat([features_top_negativas, features_top_positivas]).sort_values(by='importance'), x='feature', y='importance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26JMNBsWxrC2"
      },
      "source": [
        "## Modelo SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nardWJlbyVky"
      },
      "source": [
        "**Support vector machines** (SVM) son un conjunto de métodos de aprendizaje supervisados classification, regression y outliers detection.\n",
        "\n",
        "Las ventajas de los SVM son:<br>\n",
        "  * Efectivo en espacios de altas dimensiones.\n",
        "  * Sigue siendo efectivo en casos donde el número de dimensiones es mayor que el número de muestras, pero dependera del nivel de regularizacion\n",
        "  * Utiliza un subconjunto de puntos de entrenamiento en la función de decisión (llamados vectores de soporte), por lo que también es eficiente en la memoria.\n",
        "  * Versátil: se pueden especificar diferentes funciones de Kernel para la función de decisión. Se proporcionan núcleos comunes, pero también es posible especificar núcleos personalizados.\n",
        "\n",
        "Las desventajas de los SVM son:<br>\n",
        "  * Si el número de características es mucho mayor que el número de muestras, se necesita evitar el ajuste excesivo al elegir las funciones de Kernel y el término de regularización es crucial.\n",
        "  * Los SVM no proporcionan directamente estimaciones de probabilidad, estas se calculan utilizando una costosa validación cruzada.    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp4V5dowyVBH"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train_transform, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbbyfIDw0t3z"
      },
      "source": [
        "### Resultados obtenidos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCNCd9K00tMW"
      },
      "source": [
        "y_pred = clf.predict(X_test_tranform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N--O8JPa0mjX"
      },
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9IVDeVL03cm"
      },
      "source": [
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHP1NEMR07ip"
      },
      "source": [
        "plot_roc_curve(clf,X_test_tranform,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsWXfDsZ3UWW"
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve,plot_confusion_matrix\n",
        "plot_confusion_matrix(clf,X_test_tranform,y_test,normalize='true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4TYXHHT1A9B"
      },
      "source": [
        "features = pd.DataFrame()\n",
        "features['feature'] = cv.get_feature_names()\n",
        "features['importance'] = clf.coef_.flatten()\n",
        "\n",
        "features = features.sort_values(by='importance')\n",
        "\n",
        "features_top_positivas = features[:200]\n",
        "features_top_negativas = features[-200:]\n",
        "\n",
        "px.bar(pd.concat([features_top_negativas, features_top_positivas]).sort_values(by='importance'), x='feature', y='importance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCJTK3my1RIh"
      },
      "source": [
        "## Modelo RandomForest\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4bcQiRi1Xih"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train_transform, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgMvEbEk1psp"
      },
      "source": [
        "### Resultados obtenidos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KjhhqjS11LW"
      },
      "source": [
        "y_pred = clf.predict(X_test_tranform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8hdSxvz14MU"
      },
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rIxSGdA17MA"
      },
      "source": [
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRXZwZGD3XUt"
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve,plot_confusion_matrix,plot_precision_recall_curve\n",
        "plot_confusion_matrix(clf,X_test_tranform,y_test,normalize='true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDeADygJ19mg"
      },
      "source": [
        "plot_roc_curve(clf,X_test_tranform,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3MniaMa3fqZ"
      },
      "source": [
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "\n",
        "plot_precision_recall_curve(clf,X_test_tranform,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkVI0H552OZN"
      },
      "source": [
        "features = pd.DataFrame()\n",
        "features['feature'] = cv.get_feature_names()\n",
        "features['importance'] = clf.feature_importances_\n",
        "\n",
        "features = features.sort_values(by='importance',ascending=False)\n",
        "\n",
        "px.bar(features.head(100), x='feature', y='importance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvgwTt4GLHEq"
      },
      "source": [
        "## Posibles cambios\n",
        "* Entrenar con lematizador o Stemmer\n",
        "* Buscar los hiperónimo de cada palabra\n",
        "* Eliminar símbolos, numeros, tildes, ñ o caracteres inválidos\n",
        "* Entrenar diferentes tipos de modelos -> SVM, Naive Bayes, regresión logística\n",
        "* Cross validation\n",
        "* Incluir Bigramas y Trigramas\n",
        "* Explorar técnicas para reducción de dimensionalidad y seleccion de features\n",
        "* Regularizacion\n",
        "* Vectorizacion de tokens con TF-IDF\n"
      ]
    }
  ]
}