{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EjemploClasificacionFakeNews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMvDWHo44vjP"
      },
      "source": [
        "# Ejemplo de clasificacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAuvaHjZUNNe"
      },
      "source": [
        "## Importar librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqnR8sM_UMfA"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI3hzf8c4_sN"
      },
      "source": [
        "## Exploracion Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciRdTY8QtFXM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_colwidth',300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6eAo9fM5IWF"
      },
      "source": [
        "# Importo el dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Austral/TextMining/Clases/Clase3/Practica/datos/archive.zip', encoding='utf-8') #Encoding\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S08VG-UHOSQ1"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHRm9S_itmp5"
      },
      "source": [
        "print(\"Cantidad de targets: \")\n",
        "df.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10-ygXeHCWw4"
      },
      "source": [
        "# Creo una caracteristica con el tama√±o del titulo\n",
        "df['title_text_size'] = df['title'].str.len()\n",
        "df['title_text_words_count'] = df['title'].str.split().apply(len)\n",
        "df['text_text_size'] = df['text'].str.len()\n",
        "df['text_text_words_count'] = df['text'].str.split().apply(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXhG_xYsuO2G"
      },
      "source": [
        "import plotly.express as px\n",
        "fig = px.histogram(df, x=\"title_text_words_count\", color='label',histnorm='probability')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXTg_VJO73nq"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def get_top_n_words(corpus, n=10):\n",
        "    cv = CountVectorizer() # Agregar StopWords\n",
        "    corpus_matrix = cv.fit_transform(corpus)\n",
        "    corpus_matrix = pd.DataFrame.sparse.from_spmatrix(corpus_matrix, columns=cv.get_feature_names())\n",
        "    aux = corpus_matrix.sum()\n",
        "    aux=aux.to_frame('count')\n",
        "    aux.sort_values(by='count',ascending=False,inplace=True)\n",
        "    aux=aux.head(n)\n",
        "    return aux"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GMZfcN7CjRC"
      },
      "source": [
        "# Cantidad de tokens en todo el corpus\n",
        "most_importants = get_top_n_words(df.text, n=500)\n",
        "print(most_importants.head(10))\n",
        "px.bar(most_importants, x=most_importants.index, y='count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61uLLn7O1nb7"
      },
      "source": [
        "# Cantidad de tokens en todo el corpus\n",
        "most_importants = get_top_n_words(df[df.label=='FAKE'].text, n=500)\n",
        "print(most_importants.head(10))\n",
        "px.bar(most_importants, x=most_importants.index, y='count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7bBSDeD19h0"
      },
      "source": [
        "# Cantidad de tokens en todo el corpus\n",
        "most_importants = get_top_n_words(df[df.label=='REAL'].text, n=500)\n",
        "print(most_importants.head(10))\n",
        "px.bar(most_importants, x=most_importants.index, y='count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibms-RhsEbjb"
      },
      "source": [
        "## Proceso NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO2AO7wRDVWl"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "stop_words = set( stopwords.words('english'))\n",
        "stop_words.update(string.punctuation)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Pre procesamiento basico, agregar las etapas que considere necesario\n",
        "def pre_procesamiento_texto(text):\n",
        "  # Armo los tokens para procesar los datos\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Elimino las stopwords\n",
        "  tokens = [t.lower() for t in tokens if t.lower() not in stop_words]\n",
        "\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCYat43tD1Ar"
      },
      "source": [
        "text='Daniel Greenfield, a Shillman Journalism Fellow at the Freedom Center, is a New York writer focusing on radical Islam'\n",
        "print(text)\n",
        "pre_procesamiento_texto(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7ZZiC74x4DS"
      },
      "source": [
        "## Vectorizacion resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE1Cs5_gD6Ph"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GtbLgMTJMpU"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.text, df.label,test_size=.30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WkJmfOqIorM"
      },
      "source": [
        "# Vectorizacion de resultados BOW, tambien se puede probar TF-IDF\n",
        "cv = CountVectorizer(tokenizer=pre_procesamiento_texto)\n",
        "X_train_transform = cv.fit_transform(X_train) # Armo matriz para entrenar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AowB_A800Jkm"
      },
      "source": [
        "X_train_transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwUlVz29S-Dg"
      },
      "source": [
        "df_tranform = pd.DataFrame.sparse.from_spmatrix(X_train_transform, columns=cv.get_feature_names())\n",
        "df_tranform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmuFUd2BlGQH"
      },
      "source": [
        "aux = df_tranform.sum().to_frame('count')\n",
        "aux.sort_values(by='count',ascending=False,inplace=True)\n",
        "aux=aux.head(1000)\n",
        "px.bar(aux, x=aux.index, y='count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9IyaU-Y88Ew"
      },
      "source": [
        "print(\"Vocabulario de \", len(cv.get_feature_names()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBv7KTdtLSGd"
      },
      "source": [
        "X_train_transform.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLkKF7w5Rqa3"
      },
      "source": [
        "Se puede ver que tenemos mas features que observaciones. Si se utiliza algun metodo de algebra lineal se necesitan tener al menos la misma cantidad de observaciones que de incognitas, sino existiran infinitas soluciones haciendo que el modelo no generalize correctamente. \n",
        "\n",
        "Es recomendable utilizar tecnicas de Cross \n",
        "Validation,  reduccion de dimencionalidad y seleccion de features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEi0st59Efp9"
      },
      "source": [
        "## Modelo Regresion Logistica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeeQ3F8iJ8gA"
      },
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(X_train_transform, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFH8353BEkm-"
      },
      "source": [
        "### Resultados obtenidos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwcvdgn1KDFT"
      },
      "source": [
        "X_test_tranform = cv.transform(X_test) \n",
        "y_pred = clf.predict(X_test_tranform.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZDEu3XsKQNB"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Youyd5YcKnJR"
      },
      "source": [
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdpspo0OkNoT"
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve,plot_confusion_matrix\n",
        "plot_roc_curve(clf,X_test_tranform,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_w415wJ24Nz"
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve,plot_confusion_matrix\n",
        "plot_confusion_matrix(clf,X_test_tranform,y_test,normalize='true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMVAGT-D5Pi5"
      },
      "source": [
        "features = pd.DataFrame()\n",
        "features['feature'] = cv.get_feature_names()\n",
        "features['importance'] = clf.coef_.flatten()\n",
        "\n",
        "features = features.sort_values(by='importance')\n",
        "\n",
        "features_top_positivas = features[:200]\n",
        "features_top_negativas = features[-200:]\n",
        "\n",
        "px.bar(pd.concat([features_top_negativas, features_top_positivas]).sort_values(by='importance'), x='feature', y='importance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26JMNBsWxrC2"
      },
      "source": [
        "## Modelo SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nardWJlbyVky"
      },
      "source": [
        "**Support vector machines** (SVM) son un conjunto de m√©todos de aprendizaje supervisados classification, regression y outliers detection.\n",
        "\n",
        "Las ventajas de los SVM son:<br>\n",
        "  * Efectivo en espacios de altas dimensiones.\n",
        "  * Sigue siendo efectivo en casos donde el n√∫mero de dimensiones es mayor que el n√∫mero de muestras, pero dependera del nivel de regularizacion\n",
        "  * Utiliza un subconjunto de puntos de entrenamiento en la funci√≥n de decisi√≥n (llamados vectores de soporte), por lo que tambi√©n es eficiente en la memoria.\n",
        "  * Vers√°til: se pueden especificar diferentes funciones de Kernel para la funci√≥n de decisi√≥n. Se proporcionan n√∫cleos comunes, pero tambi√©n es posible especificar n√∫cleos personalizados.\n",
        "\n",
        "Las desventajas de los SVM son:<br>\n",
        "  * Si el n√∫mero de caracter√≠sticas es mucho mayor que el n√∫mero de muestras, se necesita evitar el ajuste excesivo al elegir las funciones de Kernel y el t√©rmino de regularizaci√≥n es crucial.\n",
        "  * Los SVM no proporcionan directamente estimaciones de probabilidad, estas se calculan utilizando una costosa validaci√≥n cruzada.    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp4V5dowyVBH"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train_transform, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbbyfIDw0t3z"
      },
      "source": [
        "### Resultados obtenidos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCNCd9K00tMW"
      },
      "source": [
        "y_pred = clf.predict(X_test_tranform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N--O8JPa0mjX"
      },
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9IVDeVL03cm"
      },
      "source": [
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHP1NEMR07ip"
      },
      "source": [
        "plot_roc_curve(clf,X_test_tranform,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsWXfDsZ3UWW"
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve,plot_confusion_matrix\n",
        "plot_confusion_matrix(clf,X_test_tranform,y_test,normalize='true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4TYXHHT1A9B"
      },
      "source": [
        "features = pd.DataFrame()\n",
        "features['feature'] = cv.get_feature_names()\n",
        "features['importance'] = clf.coef_.flatten()\n",
        "\n",
        "features = features.sort_values(by='importance')\n",
        "\n",
        "features_top_positivas = features[:200]\n",
        "features_top_negativas = features[-200:]\n",
        "\n",
        "px.bar(pd.concat([features_top_negativas, features_top_positivas]).sort_values(by='importance'), x='feature', y='importance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCJTK3my1RIh"
      },
      "source": [
        "## Modelo RandomForest\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4bcQiRi1Xih"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train_transform, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgMvEbEk1psp"
      },
      "source": [
        "### Resultados obtenidos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KjhhqjS11LW"
      },
      "source": [
        "y_pred = clf.predict(X_test_tranform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8hdSxvz14MU"
      },
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rIxSGdA17MA"
      },
      "source": [
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRXZwZGD3XUt"
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve,plot_confusion_matrix,plot_precision_recall_curve\n",
        "plot_confusion_matrix(clf,X_test_tranform,y_test,normalize='true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDeADygJ19mg"
      },
      "source": [
        "plot_roc_curve(clf,X_test_tranform,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3MniaMa3fqZ"
      },
      "source": [
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "\n",
        "plot_precision_recall_curve(clf,X_test_tranform,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkVI0H552OZN"
      },
      "source": [
        "features = pd.DataFrame()\n",
        "features['feature'] = cv.get_feature_names()\n",
        "features['importance'] = clf.feature_importances_\n",
        "\n",
        "features = features.sort_values(by='importance',ascending=False)\n",
        "\n",
        "px.bar(features.head(100), x='feature', y='importance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvgwTt4GLHEq"
      },
      "source": [
        "## Posibles cambios\n",
        "* Entrenar con lematizador o Stemmer\n",
        "* Buscar los hiper√≥nimo de cada palabra\n",
        "* Eliminar s√≠mbolos, numeros, tildes, √± o caracteres inv√°lidos\n",
        "* Entrenar diferentes tipos de modelos -> SVM, Naive Bayes, regresi√≥n log√≠stica\n",
        "* Cross validation\n",
        "* Incluir Bigramas y Trigramas\n",
        "* Explorar t√©cnicas para reducci√≥n de dimensionalidad y seleccion de features\n",
        "* Regularizacion\n",
        "* Vectorizacion de tokens con TF-IDF\n"
      ]
    }
  ]
}